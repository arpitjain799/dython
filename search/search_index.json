{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Dython \u00b6 Welcome! \u00b6 Dython is a set of D ata analysis tools in p YTHON 3.x, which can let you get more insights about your data. This library was designed with analysis usage in mind - meaning ease-of-use, functionality and readability are the core values of this library. Production-grade performance, on the other hand, were not considered. Here are some cool things you can do with it: Given a dataset, Dython will automatically find which features are categorical and which are numerical, compute a relevant measure of association between each and every feature, and plot it all as an easy-to-read heat-map. And all this is done with a single line: from dython.nominal import associations associations ( data ) The result: Here's another thing - given a machine-learning multi-class model's predictions, you can easily display each class' ROC curve, AUC score and find the estimated-optimal thresholds - again, with a single line of code: from dython.model_utils import metric_graph metric_graph ( y_true , y_pred , metric = 'roc' ) The result: Installation \u00b6 Dython can be installed directly using pip : pip install dython Other installation options are available, see the installation page for more information. Examples \u00b6 See some usage examples of nominal.associations and model_utils.roc_graph on the examples page . All examples can also be imported and executed from dython.examples . Modules Documentation \u00b6 Full documentation of all modues and public methods is available: data_utils nominal model_utils sampling","title":"Home"},{"location":"#dython","text":"","title":"Dython"},{"location":"#welcome","text":"Dython is a set of D ata analysis tools in p YTHON 3.x, which can let you get more insights about your data. This library was designed with analysis usage in mind - meaning ease-of-use, functionality and readability are the core values of this library. Production-grade performance, on the other hand, were not considered. Here are some cool things you can do with it: Given a dataset, Dython will automatically find which features are categorical and which are numerical, compute a relevant measure of association between each and every feature, and plot it all as an easy-to-read heat-map. And all this is done with a single line: from dython.nominal import associations associations ( data ) The result: Here's another thing - given a machine-learning multi-class model's predictions, you can easily display each class' ROC curve, AUC score and find the estimated-optimal thresholds - again, with a single line of code: from dython.model_utils import metric_graph metric_graph ( y_true , y_pred , metric = 'roc' ) The result:","title":"Welcome!"},{"location":"#installation","text":"Dython can be installed directly using pip : pip install dython Other installation options are available, see the installation page for more information.","title":"Installation"},{"location":"#examples","text":"See some usage examples of nominal.associations and model_utils.roc_graph on the examples page . All examples can also be imported and executed from dython.examples .","title":"Examples"},{"location":"#modules-documentation","text":"Full documentation of all modues and public methods is available: data_utils nominal model_utils sampling","title":"Modules Documentation"},{"location":"related_blogposts/","text":"Related Blogposts \u00b6 Here are some blogposts I wrote, explaining and using some of the methods of Dython: Read more about the categorical tools on The Search for Categorical Correlation Read more about using ROC graphs on Hard ROC: Really Understanding & Properly Using ROC and AUC Read more about KS Area Between Curves and when not to use ROC graphs (and other common metrics) on The Metric System: How to Correctly Measure Your Model","title":"Related Blogosts"},{"location":"related_blogposts/#related-blogposts","text":"Here are some blogposts I wrote, explaining and using some of the methods of Dython: Read more about the categorical tools on The Search for Categorical Correlation Read more about using ROC graphs on Hard ROC: Really Understanding & Properly Using ROC and AUC Read more about KS Area Between Curves and when not to use ROC graphs (and other common metrics) on The Metric System: How to Correctly Measure Your Model","title":"Related Blogposts"},{"location":"getting_started/examples/","text":"Examples \u00b6 Examples can be imported and executed from dython.examples . associations_iris_example() \u00b6 Plot an example of an associations heat-map of the Iris dataset features. All features of this dataset are numerical (except for the target). Example code: import pandas as pd from sklearn import datasets from dython.nominal import associations # Load data iris = datasets . load_iris () # Convert int classes to strings to allow associations # method to automatically recognize categorical columns target = [ 'C {} ' . format ( i ) for i in iris . target ] # Prepare data X = pd . DataFrame ( data = iris . data , columns = iris . feature_names ) y = pd . DataFrame ( data = target , columns = [ 'target' ]) df = pd . concat ([ X , y ], axis = 1 ) # Plot features associations associations ( df ) Output: associations_mushrooms_example() \u00b6 Plot an example of an associations heat-map of the UCI Mushrooms dataset features. All features of this dataset are categorical. This example will use Theil's U. Example code: import pandas as pd from dython.nominal import associations # Download and load data from UCI df = pd . read_csv ( 'http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data' ) df . columns = [ 'class' , 'cap-shape' , 'cap-surface' , 'cap-color' , 'bruises' , 'odor' , 'gill-attachment' , 'gill-spacing' , 'gill-size' , 'gill-color' , 'stalk-shape' , 'stalk-root' , 'stalk-surface-above-ring' , 'stalk-surface-below-ring' , 'stalk-color-above-ring' , 'stalk-color-below-ring' , 'veil-type' , 'veil-color' , 'ring-number' , 'ring-type' , 'spore-print-color' , 'population' , 'habitat' ] # Plot features associations associations ( df , nom_nom_assoc = 'theil' , figsize = ( 15 , 15 )) Output: ks_abc_example() \u00b6 An example of KS Area Between Curve of a simple binary classifier trained over the Breast Cancer dataset. Example code: from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from dython.model_utils import ks_abc # Load and split data data = datasets . load_breast_cancer () X_train , X_test , y_train , y_test = train_test_split ( data . data , data . target , test_size = .5 , random_state = 0 ) # Train model and predict model = LogisticRegression ( solver = 'liblinear' ) model . fit ( X_train , y_train ) y_pred = model . predict_proba ( X_test ) # Perform KS test and compute area between curves ks_abc ( y_test , y_pred [:, 1 ]) Output: pr_graph_example() \u00b6 Plot an example Precision-Recall graph of an SVM model predictions over the Iris dataset. Example code: import numpy as np from sklearn import svm , datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import label_binarize from sklearn.multiclass import OneVsRestClassifier from dython.model_utils import metric_graph # Load data iris = datasets . load_iris () X = iris . data y = label_binarize ( iris . target , classes = [ 0 , 1 , 2 ]) # Add noisy features random_state = np . random . RandomState ( 4 ) n_samples , n_features = X . shape X = np . c_ [ X , random_state . randn ( n_samples , 200 * n_features )] # Train a model X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = .5 , random_state = 0 ) classifier = OneVsRestClassifier ( svm . SVC ( kernel = 'linear' , probability = True , random_state = 0 )) # Predict y_score = classifier . fit ( X_train , y_train ) . predict_proba ( X_test ) # Plot ROC graphs metric_graph ( y_test , y_score , 'pr' , class_names = iris . target_names ) Output: roc_graph_example() \u00b6 Plot an example ROC graph of an SVM model predictions over the Iris dataset. Based on sklearn examples (as was seen on April 2018). Example code: import numpy as np from sklearn import svm , datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import label_binarize from sklearn.multiclass import OneVsRestClassifier from dython.model_utils import metric_graph # Load data iris = datasets . load_iris () X = iris . data y = label_binarize ( iris . target , classes = [ 0 , 1 , 2 ]) # Add noisy features random_state = np . random . RandomState ( 4 ) n_samples , n_features = X . shape X = np . c_ [ X , random_state . randn ( n_samples , 200 * n_features )] # Train a model X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = .5 , random_state = 0 ) classifier = OneVsRestClassifier ( svm . SVC ( kernel = 'linear' , probability = True , random_state = 0 )) # Predict y_score = classifier . fit ( X_train , y_train ) . predict_proba ( X_test ) # Plot ROC graphs metric_graph ( y_test , y_score , 'roc' , class_names = iris . target_names ) Output: Note: Due to the nature of np.random.RandomState which is used in this example, the output graph may vary from one machine to another. split_hist_example() \u00b6 Plot an example of split histogram of data from the breast-cancer dataset. While this example presents a numerical column split by a categorical one, categorical columns can also be used as the values, as well as numerical columns as the split criteria. Example code: import pandas as pd from sklearn import datasets from dython.data_utils import split_hist # Load data and convert to DataFrame data = datasets . load_breast_cancer () df = pd . DataFrame ( data = data . data , columns = data . feature_names ) df [ 'malignant' ] = [ not bool ( x ) for x in data . target ] # Plot histogram split_hist ( df , 'mean radius' , split_by = 'malignant' , bins = 20 , figsize = ( 15 , 7 )) Output:","title":"Examples"},{"location":"getting_started/examples/#examples","text":"Examples can be imported and executed from dython.examples .","title":"Examples"},{"location":"getting_started/examples/#associations_iris_example","text":"Plot an example of an associations heat-map of the Iris dataset features. All features of this dataset are numerical (except for the target). Example code: import pandas as pd from sklearn import datasets from dython.nominal import associations # Load data iris = datasets . load_iris () # Convert int classes to strings to allow associations # method to automatically recognize categorical columns target = [ 'C {} ' . format ( i ) for i in iris . target ] # Prepare data X = pd . DataFrame ( data = iris . data , columns = iris . feature_names ) y = pd . DataFrame ( data = target , columns = [ 'target' ]) df = pd . concat ([ X , y ], axis = 1 ) # Plot features associations associations ( df ) Output:","title":"associations_iris_example()"},{"location":"getting_started/examples/#associations_mushrooms_example","text":"Plot an example of an associations heat-map of the UCI Mushrooms dataset features. All features of this dataset are categorical. This example will use Theil's U. Example code: import pandas as pd from dython.nominal import associations # Download and load data from UCI df = pd . read_csv ( 'http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data' ) df . columns = [ 'class' , 'cap-shape' , 'cap-surface' , 'cap-color' , 'bruises' , 'odor' , 'gill-attachment' , 'gill-spacing' , 'gill-size' , 'gill-color' , 'stalk-shape' , 'stalk-root' , 'stalk-surface-above-ring' , 'stalk-surface-below-ring' , 'stalk-color-above-ring' , 'stalk-color-below-ring' , 'veil-type' , 'veil-color' , 'ring-number' , 'ring-type' , 'spore-print-color' , 'population' , 'habitat' ] # Plot features associations associations ( df , nom_nom_assoc = 'theil' , figsize = ( 15 , 15 )) Output:","title":"associations_mushrooms_example()"},{"location":"getting_started/examples/#ks_abc_example","text":"An example of KS Area Between Curve of a simple binary classifier trained over the Breast Cancer dataset. Example code: from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from dython.model_utils import ks_abc # Load and split data data = datasets . load_breast_cancer () X_train , X_test , y_train , y_test = train_test_split ( data . data , data . target , test_size = .5 , random_state = 0 ) # Train model and predict model = LogisticRegression ( solver = 'liblinear' ) model . fit ( X_train , y_train ) y_pred = model . predict_proba ( X_test ) # Perform KS test and compute area between curves ks_abc ( y_test , y_pred [:, 1 ]) Output:","title":"ks_abc_example()"},{"location":"getting_started/examples/#pr_graph_example","text":"Plot an example Precision-Recall graph of an SVM model predictions over the Iris dataset. Example code: import numpy as np from sklearn import svm , datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import label_binarize from sklearn.multiclass import OneVsRestClassifier from dython.model_utils import metric_graph # Load data iris = datasets . load_iris () X = iris . data y = label_binarize ( iris . target , classes = [ 0 , 1 , 2 ]) # Add noisy features random_state = np . random . RandomState ( 4 ) n_samples , n_features = X . shape X = np . c_ [ X , random_state . randn ( n_samples , 200 * n_features )] # Train a model X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = .5 , random_state = 0 ) classifier = OneVsRestClassifier ( svm . SVC ( kernel = 'linear' , probability = True , random_state = 0 )) # Predict y_score = classifier . fit ( X_train , y_train ) . predict_proba ( X_test ) # Plot ROC graphs metric_graph ( y_test , y_score , 'pr' , class_names = iris . target_names ) Output:","title":"pr_graph_example()"},{"location":"getting_started/examples/#roc_graph_example","text":"Plot an example ROC graph of an SVM model predictions over the Iris dataset. Based on sklearn examples (as was seen on April 2018). Example code: import numpy as np from sklearn import svm , datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import label_binarize from sklearn.multiclass import OneVsRestClassifier from dython.model_utils import metric_graph # Load data iris = datasets . load_iris () X = iris . data y = label_binarize ( iris . target , classes = [ 0 , 1 , 2 ]) # Add noisy features random_state = np . random . RandomState ( 4 ) n_samples , n_features = X . shape X = np . c_ [ X , random_state . randn ( n_samples , 200 * n_features )] # Train a model X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = .5 , random_state = 0 ) classifier = OneVsRestClassifier ( svm . SVC ( kernel = 'linear' , probability = True , random_state = 0 )) # Predict y_score = classifier . fit ( X_train , y_train ) . predict_proba ( X_test ) # Plot ROC graphs metric_graph ( y_test , y_score , 'roc' , class_names = iris . target_names ) Output: Note: Due to the nature of np.random.RandomState which is used in this example, the output graph may vary from one machine to another.","title":"roc_graph_example()"},{"location":"getting_started/examples/#split_hist_example","text":"Plot an example of split histogram of data from the breast-cancer dataset. While this example presents a numerical column split by a categorical one, categorical columns can also be used as the values, as well as numerical columns as the split criteria. Example code: import pandas as pd from sklearn import datasets from dython.data_utils import split_hist # Load data and convert to DataFrame data = datasets . load_breast_cancer () df = pd . DataFrame ( data = data . data , columns = data . feature_names ) df [ 'malignant' ] = [ not bool ( x ) for x in data . target ] # Plot histogram split_hist ( df , 'mean radius' , split_by = 'malignant' , bins = 20 , figsize = ( 15 , 7 )) Output:","title":"split_hist_example()"},{"location":"getting_started/installation/","text":"Installing Dython \u00b6 Installation \u00b6 The easiest way to install dython is using pip install : pip install dython Or, via the conda package manager: conda install -c conda-forge dython If you'd like to use the source code instead, you can install directly from it using any of the following methods: Install source code using pip: pip install git+https://github.com/shakedzy/dython.git ` Download the source code as a ZIP file Download the source code as a TAR ball Dependencies \u00b6 Dython requires Python 3.5 or higher, and the following packages: numpy pandas seaborn scipy matplotlib sklearn scikit-plot","title":"Installation"},{"location":"getting_started/installation/#installing-dython","text":"","title":"Installing Dython"},{"location":"getting_started/installation/#installation","text":"The easiest way to install dython is using pip install : pip install dython Or, via the conda package manager: conda install -c conda-forge dython If you'd like to use the source code instead, you can install directly from it using any of the following methods: Install source code using pip: pip install git+https://github.com/shakedzy/dython.git ` Download the source code as a ZIP file Download the source code as a TAR ball","title":"Installation"},{"location":"getting_started/installation/#dependencies","text":"Dython requires Python 3.5 or higher, and the following packages: numpy pandas seaborn scipy matplotlib sklearn scikit-plot","title":"Dependencies"},{"location":"modules/data_utils/","text":"data_utils \u00b6 identify_columns_with_na \u00b6 identify_columns_with_na(dataset) Given a dataset, return columns names having NA values, sorted in descending order by their number of NAs. dataset : np.ndarray / pd.DataFrame Returns: A pd.DataFrame of two columns ( ['column', 'na_count'] ), consisting of only the names of columns with NA values, sorted by their number of NA values. Example: >>> df = pd . DataFrame ({ 'col1' : [ 'a' , np . nan , 'a' , 'a' ], 'col2' : [ 3 , np . nan , 2 , np . nan ], 'col3' : [ 1. , 2. , 3. , 4. ]}) >>> identify_columns_with_na ( df ) column na_count 1 col2 2 0 col1 1 identify_columns_by_type \u00b6 identify_columns_by_type(dataset, include) Given a dataset, identify columns of the types requested. dataset : np.ndarray / pd.DataFrame include : list which column types to filter by. Returns: list of categorical columns Example: >>> df = pd . DataFrame ({ 'col1' : [ 'a' , 'b' , 'c' , 'a' ], 'col2' : [ 3 , 4 , 2 , 1 ], 'col3' : [ 1. , 2. , 3. , 4. ]}) >>> identify_columns_by_type ( df , include = [ 'int64' , 'float64' ]) [ 'col2' , 'col3' ] one_hot_encode \u00b6 one_hot_encode(arr, classes=None) One-hot encode a 1D array. Based on this StackOverflow answer . arr : array-like An array to be one-hot encoded. Must contain only non-negative integers classes : int or None number of classes. if None, max value of the array will be used Returns: 2D one-hot encoded array Example: >>> one_hot_encode ([ 1 , 0 , 5 ]) [[ 0. 1. 0. 0. 0. 0. ] [ 1. 0. 0. 0. 0. 0. ] [ 0. 0. 0. 0. 0. 1. ]] split_hist \u00b6 split_hist(dataset, values, split_by, title='', xlabel='', ylabel=None, figsize=None, legend='best', plot=True, **hist_kwargs) Plot a histogram of values from a given dataset, split by the values of a chosen column dataset : pd.DataFrame values : string The column name of the values to be displayed in the histogram split_by : string The column name of the values to split the histogram by title : string or None , default = '' The plot's title. If empty string, will be '{values} by {split_by}' xlabel : string or None , default = '' x-axis label. If empty string, will be '{values}' ylabel : string or None , default: None y-axis label figsize : ( int , int ) or None , default = None A Matplotlib figure-size tuple. If None , falls back to Matplotlib's default. legend : string or None , default = 'best' A Matplotlib legend location string. See Matplotlib documentation for possible options plot : Boolean , default = True Plot the histogram hist_kwargs : key-value pairs A key-value pairs to be passed to Matplotlib hist method. See Matplotlib documentation for possible options Returns: A Matplotlib Axe Example: See examples .","title":"data_utils"},{"location":"modules/data_utils/#data_utils","text":"","title":"data_utils"},{"location":"modules/data_utils/#identify_columns_with_na","text":"identify_columns_with_na(dataset) Given a dataset, return columns names having NA values, sorted in descending order by their number of NAs. dataset : np.ndarray / pd.DataFrame Returns: A pd.DataFrame of two columns ( ['column', 'na_count'] ), consisting of only the names of columns with NA values, sorted by their number of NA values. Example: >>> df = pd . DataFrame ({ 'col1' : [ 'a' , np . nan , 'a' , 'a' ], 'col2' : [ 3 , np . nan , 2 , np . nan ], 'col3' : [ 1. , 2. , 3. , 4. ]}) >>> identify_columns_with_na ( df ) column na_count 1 col2 2 0 col1 1","title":"identify_columns_with_na"},{"location":"modules/data_utils/#identify_columns_by_type","text":"identify_columns_by_type(dataset, include) Given a dataset, identify columns of the types requested. dataset : np.ndarray / pd.DataFrame include : list which column types to filter by. Returns: list of categorical columns Example: >>> df = pd . DataFrame ({ 'col1' : [ 'a' , 'b' , 'c' , 'a' ], 'col2' : [ 3 , 4 , 2 , 1 ], 'col3' : [ 1. , 2. , 3. , 4. ]}) >>> identify_columns_by_type ( df , include = [ 'int64' , 'float64' ]) [ 'col2' , 'col3' ]","title":"identify_columns_by_type"},{"location":"modules/data_utils/#one_hot_encode","text":"one_hot_encode(arr, classes=None) One-hot encode a 1D array. Based on this StackOverflow answer . arr : array-like An array to be one-hot encoded. Must contain only non-negative integers classes : int or None number of classes. if None, max value of the array will be used Returns: 2D one-hot encoded array Example: >>> one_hot_encode ([ 1 , 0 , 5 ]) [[ 0. 1. 0. 0. 0. 0. ] [ 1. 0. 0. 0. 0. 0. ] [ 0. 0. 0. 0. 0. 1. ]]","title":"one_hot_encode"},{"location":"modules/data_utils/#split_hist","text":"split_hist(dataset, values, split_by, title='', xlabel='', ylabel=None, figsize=None, legend='best', plot=True, **hist_kwargs) Plot a histogram of values from a given dataset, split by the values of a chosen column dataset : pd.DataFrame values : string The column name of the values to be displayed in the histogram split_by : string The column name of the values to split the histogram by title : string or None , default = '' The plot's title. If empty string, will be '{values} by {split_by}' xlabel : string or None , default = '' x-axis label. If empty string, will be '{values}' ylabel : string or None , default: None y-axis label figsize : ( int , int ) or None , default = None A Matplotlib figure-size tuple. If None , falls back to Matplotlib's default. legend : string or None , default = 'best' A Matplotlib legend location string. See Matplotlib documentation for possible options plot : Boolean , default = True Plot the histogram hist_kwargs : key-value pairs A key-value pairs to be passed to Matplotlib hist method. See Matplotlib documentation for possible options Returns: A Matplotlib Axe Example: See examples .","title":"split_hist"},{"location":"modules/model_utils/","text":"model_utils \u00b6 ks_abc \u00b6 ks_abc(y_true, y_pred, ax=None, figsize=None, colors=('darkorange', 'b'), title=None, xlim=(0.,1.), ylim=(0.,1.), fmt='.2f', lw=2, legend='best', plot=True, filename=None) Perform the Kolmogorov\u2013Smirnov test over the positive and negative distributions of a binary classifier, and compute the area between curves. The KS test plots the fraction of positives and negatives predicted correctly below each threshold. It then finds the optimal threshold, being the one enabling the best class separation. The area between curves allows a better insight into separation. The higher the area is (1 being the maximum), the more the positive and negative distributions' center-of-mass are closer to 1 and 0, respectively. Based on scikit-plot plot_ks_statistic method. y_true : array-like The true labels of the dataset y_pred : array-like The probabilities predicted by a binary classifier ax : matplotlib ax Default: None Matplotlib Axis on which the curves will be plotted figsize : (int,int) or None Default: None a Matplotlib figure-size tuple. If None , falls back to Matplotlib's default. Only used if ax=None colors : list of Matplotlib color strings Default: ('darkorange', 'b') List of colors to be used for the plotted curves title : string or None Default: None Plotted graph title. If None , default title is used xlim : (float, float) Default: (0.,1.) X-axis limits. ylim : (float,float) Default: (0.,1.) Y-axis limits. fmt : string Default: '.2f' String formatting of displayed numbers. lw : int Default: 2 Line-width. legend : string or None Default: 'best' A Matplotlib legend location string. See Matplotlib documentation for possible options plot : Boolean , default = True Plot the KS curves filename : string or None Default: None If not None, plot will be saved to the given file name. Returns: A dictionary of the following keys: abc : area between curves ks_stat : computed statistic of the KS test eopt : estimated optimal threshold ax : the ax used to plot the curves Example: See examples . metric_graph \u00b6 metric_graph(y_true, y_pred, metric, micro=True, macro=True, eoptimal_threshold=True, class_names=None, colors=None, ax=None, figsize=None, xlim=(0.,1.), ylim=(0.,1.02), lw=2, ls='-', ms=10, fmt='.2f', title=None, filename=None, force_multiclass=False) Plot a metric graph of predictor's results (including AUC scores), where each row of y_true and y_pred represent a single example. ROC: Plots true-positive rate as a function of the false-positive rate of the positive label in a binary classification, where \\(TPR = TP / (TP + FN)\\) and \\(FPR = FP / (FP + TN)\\) . A naive algorithm will display a linear line going from (0,0) to (1,1), therefore having an area under-curve (AUC) of 0.5. Precision-Recall: Plots precision as a function of recall of the positive label in a binary classification, where \\(Precision = TP / (TP + FP)\\) and \\(Recall = TP / (TP + FN)\\) . A naive algorithm will display a horizontal linear line with precision of the ratio of positive examples in the dataset. Based on scikit-learn examples (as was seen on April 2018): y_true : list / NumPy ndarray The true classes of the predicted data. If only one or two columns exist, the data is treated as a binary classification (see input example below). If there are more than 2 columns, each column is considered a unique class, and a ROC graph and AUC score will be computed for each. y_pred : list / NumPy ndarray The predicted classes. Must have the same shape as y_true . metric : string The metric graph to plot. Currently supported: 'roc' for Receiver Operating Characteristic curve and 'pr' for Precision-Recall curve micro : Boolean Default: True Whether to calculate a Micro graph (not applicable for binary cases) macro : Boolean Default: True Whether to calculate a Macro graph (ROC metric only, not applicable for binary cases) eopt : Boolean Default: True Whether to calculate and display the estimated-optimal threshold for each metric graph. For ROC curves, the estimated-optimal threshold is the closest computed threshold with (fpr,tpr) values closest to (0,1). For PR curves, it is the closest one to (1,1) (perfect recall and precision) class_names : list or string Default: None Names of the different classes. In a multi-class classification, the order must match the order of the classes probabilities in the input data. In a binary classification, can be a string or a list. If a list, only the last element will be used. colors : list of Matplotlib color strings or None Default: None List of colors to be used for the plotted curves. If None , falls back to a predefined default. ax : matplotlib ax Default: None Matplotlib Axis on which the curves will be plotted figsize : (int,int) or None Default: None A Matplotlib figure-size tuple. If None , falls back to Matplotlib's default. Only used if ax=None . xlim : (float, float) Default: (0.,1.) X-axis limits. ylim : (float,float) Default: (0.,1.02) Y-axis limits. lw : int Default: 2 Line-width. ls : string Default: '-' Matplotlib line-style string ms : int Default: 10 Marker-size. fmt : string Default: '.2f' String formatting of displayed AUC and threshold numbers. legend : string or None Default: 'best' A Matplotlib legend location string. See Matplotlib documentation for possible options plot : Boolean , default = True Plot the histogram title : string or None Default: None Plotted graph title. If None, default title is used. filename : string or None Default: None If not None, plot will be saved to the given file name. force_multiclass : Boolean Default: False Only applicable if y_true and y_pred have two columns. If so, consider the data as a multiclass data rather than binary (useful when plotting curves of different models one against the other) Returns: A dictionary, one key for each class. Each value is another dictionary, holding AUC and eOpT values. Example: See examples . Binary Classification Input Example: Consider a data-set of two data-points where the true class of the first line is class 0, which was predicted with a probability of 0.6, and the second line's true class is 1, with predicted probability of 0.8. # First option: >>> metric_graph ( y_true = [ 0 , 1 ], y_pred = [ 0.6 , 0.8 ], metric = 'roc' ) # Second option: >>> metric_graph ( y_true = [[ 1 , 0 ],[ 0 , 1 ]], y_pred = [[ 0.6 , 0.4 ],[ 0.2 , 0.8 ]], metric = 'roc' ) # Both yield the same result random_forest_feature_importance \u00b6 random_forest_feature_importance(forest, features, precision=4) Given a trained sklearn.ensemble.RandomForestClassifier , plot the different features based on their importance according to the classifier, from the most important to the least. forest : sklearn.ensemble.RandomForestClassifier A trained RandomForestClassifier features : list A list of the names of the features the classifier was trained on, ordered by the same order the appeared in the training data precision : int Default: 4 Precision of feature importance.","title":"model_utils"},{"location":"modules/model_utils/#model_utils","text":"","title":"model_utils"},{"location":"modules/model_utils/#ks_abc","text":"ks_abc(y_true, y_pred, ax=None, figsize=None, colors=('darkorange', 'b'), title=None, xlim=(0.,1.), ylim=(0.,1.), fmt='.2f', lw=2, legend='best', plot=True, filename=None) Perform the Kolmogorov\u2013Smirnov test over the positive and negative distributions of a binary classifier, and compute the area between curves. The KS test plots the fraction of positives and negatives predicted correctly below each threshold. It then finds the optimal threshold, being the one enabling the best class separation. The area between curves allows a better insight into separation. The higher the area is (1 being the maximum), the more the positive and negative distributions' center-of-mass are closer to 1 and 0, respectively. Based on scikit-plot plot_ks_statistic method. y_true : array-like The true labels of the dataset y_pred : array-like The probabilities predicted by a binary classifier ax : matplotlib ax Default: None Matplotlib Axis on which the curves will be plotted figsize : (int,int) or None Default: None a Matplotlib figure-size tuple. If None , falls back to Matplotlib's default. Only used if ax=None colors : list of Matplotlib color strings Default: ('darkorange', 'b') List of colors to be used for the plotted curves title : string or None Default: None Plotted graph title. If None , default title is used xlim : (float, float) Default: (0.,1.) X-axis limits. ylim : (float,float) Default: (0.,1.) Y-axis limits. fmt : string Default: '.2f' String formatting of displayed numbers. lw : int Default: 2 Line-width. legend : string or None Default: 'best' A Matplotlib legend location string. See Matplotlib documentation for possible options plot : Boolean , default = True Plot the KS curves filename : string or None Default: None If not None, plot will be saved to the given file name. Returns: A dictionary of the following keys: abc : area between curves ks_stat : computed statistic of the KS test eopt : estimated optimal threshold ax : the ax used to plot the curves Example: See examples .","title":"ks_abc"},{"location":"modules/model_utils/#metric_graph","text":"metric_graph(y_true, y_pred, metric, micro=True, macro=True, eoptimal_threshold=True, class_names=None, colors=None, ax=None, figsize=None, xlim=(0.,1.), ylim=(0.,1.02), lw=2, ls='-', ms=10, fmt='.2f', title=None, filename=None, force_multiclass=False) Plot a metric graph of predictor's results (including AUC scores), where each row of y_true and y_pred represent a single example. ROC: Plots true-positive rate as a function of the false-positive rate of the positive label in a binary classification, where \\(TPR = TP / (TP + FN)\\) and \\(FPR = FP / (FP + TN)\\) . A naive algorithm will display a linear line going from (0,0) to (1,1), therefore having an area under-curve (AUC) of 0.5. Precision-Recall: Plots precision as a function of recall of the positive label in a binary classification, where \\(Precision = TP / (TP + FP)\\) and \\(Recall = TP / (TP + FN)\\) . A naive algorithm will display a horizontal linear line with precision of the ratio of positive examples in the dataset. Based on scikit-learn examples (as was seen on April 2018): y_true : list / NumPy ndarray The true classes of the predicted data. If only one or two columns exist, the data is treated as a binary classification (see input example below). If there are more than 2 columns, each column is considered a unique class, and a ROC graph and AUC score will be computed for each. y_pred : list / NumPy ndarray The predicted classes. Must have the same shape as y_true . metric : string The metric graph to plot. Currently supported: 'roc' for Receiver Operating Characteristic curve and 'pr' for Precision-Recall curve micro : Boolean Default: True Whether to calculate a Micro graph (not applicable for binary cases) macro : Boolean Default: True Whether to calculate a Macro graph (ROC metric only, not applicable for binary cases) eopt : Boolean Default: True Whether to calculate and display the estimated-optimal threshold for each metric graph. For ROC curves, the estimated-optimal threshold is the closest computed threshold with (fpr,tpr) values closest to (0,1). For PR curves, it is the closest one to (1,1) (perfect recall and precision) class_names : list or string Default: None Names of the different classes. In a multi-class classification, the order must match the order of the classes probabilities in the input data. In a binary classification, can be a string or a list. If a list, only the last element will be used. colors : list of Matplotlib color strings or None Default: None List of colors to be used for the plotted curves. If None , falls back to a predefined default. ax : matplotlib ax Default: None Matplotlib Axis on which the curves will be plotted figsize : (int,int) or None Default: None A Matplotlib figure-size tuple. If None , falls back to Matplotlib's default. Only used if ax=None . xlim : (float, float) Default: (0.,1.) X-axis limits. ylim : (float,float) Default: (0.,1.02) Y-axis limits. lw : int Default: 2 Line-width. ls : string Default: '-' Matplotlib line-style string ms : int Default: 10 Marker-size. fmt : string Default: '.2f' String formatting of displayed AUC and threshold numbers. legend : string or None Default: 'best' A Matplotlib legend location string. See Matplotlib documentation for possible options plot : Boolean , default = True Plot the histogram title : string or None Default: None Plotted graph title. If None, default title is used. filename : string or None Default: None If not None, plot will be saved to the given file name. force_multiclass : Boolean Default: False Only applicable if y_true and y_pred have two columns. If so, consider the data as a multiclass data rather than binary (useful when plotting curves of different models one against the other) Returns: A dictionary, one key for each class. Each value is another dictionary, holding AUC and eOpT values. Example: See examples . Binary Classification Input Example: Consider a data-set of two data-points where the true class of the first line is class 0, which was predicted with a probability of 0.6, and the second line's true class is 1, with predicted probability of 0.8. # First option: >>> metric_graph ( y_true = [ 0 , 1 ], y_pred = [ 0.6 , 0.8 ], metric = 'roc' ) # Second option: >>> metric_graph ( y_true = [[ 1 , 0 ],[ 0 , 1 ]], y_pred = [[ 0.6 , 0.4 ],[ 0.2 , 0.8 ]], metric = 'roc' ) # Both yield the same result","title":"metric_graph"},{"location":"modules/model_utils/#random_forest_feature_importance","text":"random_forest_feature_importance(forest, features, precision=4) Given a trained sklearn.ensemble.RandomForestClassifier , plot the different features based on their importance according to the classifier, from the most important to the least. forest : sklearn.ensemble.RandomForestClassifier A trained RandomForestClassifier features : list A list of the names of the features the classifier was trained on, ordered by the same order the appeared in the training data precision : int Default: 4 Precision of feature importance.","title":"random_forest_feature_importance"},{"location":"modules/nominal/","text":"nominal \u00b6 associations \u00b6 associations(dataset, nominal_columns='auto', numerical_columns=None, mark_columns=False,nom_nom_assoc='cramer', num_num_assoc='pearson', nom_num_assoc='correlation_ratio', symmetric_nom_nom=True, symmetric_num_num=True, display_rows='all', display_columns='all', hide_rows=None, hide_columns=None, cramers_v_bias_correction=True, nan_strategy=_REPLACE, nan_replace_value=_DEFAULT_REPLACE_VALUE, ax=None, figsize=None, annot=True, fmt='.2f', cmap=None, sv_color='silver', cbar=True, vmax=1.0, vmin=None, plot=True, compute_only=False, clustering=False, title=None, filename=None) Calculate the correlation/strength-of-association of features in data-set with both categorical and continuous features using: * Pearson's R for continuous-continuous cases * Correlation Ratio for categorical-continuous cases * Cramer's V or Theil's U for categorical-categorical cases dataset : NumPy ndarray / Pandas DataFrame The data-set for which the features' correlation is computed nominal_columns : string / list / NumPy ndarray Default: 'auto' Names of columns of the data-set which hold categorical values. Can also be the string 'all' to state that all columns are categorical, 'auto' (default) to identify nominal columns automatically, or None to state none are categorical. Only used if numerical_columns is None . numerical_columns : string / list / NumPy ndarray Default: None To be used instead of nominal_columns . Names of columns of the data-set which hold numerical values. Can also be the string 'all' to state that all columns are numerical (equivalent to nominal_columns=None ) or 'auto' to try to identify numerical columns (equivalent to nominal_columns=auto ). If None , nominal_columns is used. mark_columns : Boolean Default: False if True, output's columns' names will have a suffix of '(nom)' or '(con)' based on their type (nominal or continuous), as provided by nominal_columns nom_nom_assoc : callable / string Default: 'cramer' Method signature change This replaces the theil_u flag which was used till version 0.6.6. If callable, a function which recieves two pd.Series and returns a single number. If string, name of nominal-nominal (categorical-categorical) association to use: cramer : Cramer's V theil : Theil's U. When selected, heat-map columns are the provided information (meaning: \\(U = U(row|col)\\) ) num_num_assoc : callable / string Default: 'pearson' If callable, a function which recieves two pd.Series and returns a single number. If string, name of numerical-numerical association to use: pearson : Pearson's R spearman : Spearman's R kendall : Kendall's Tau nom_num_assoc : callable / string Default: 'correlation_ratio' If callable, a function which recieves two pd.Series and returns a single number. If string, name of nominal-numerical association to use: correlation_ratio : correlation ratio symmetric_nom_nom : Boolean Default: True Relevant only if nom_nom_assoc is a callable. If so, declare whether the function is symmetric ( \\(f(x,y) = f(y,x)\\) ). If False, heat-map values should be interpreted as \\(f(row,col)\\) . symmetric_num_num : Boolean Default: True Relevant only if num_num_assoc is a callable. If so, declare whether the function is symmetric ( \\(f(x,y) = f(y,x)\\) ). If False, heat-map values should be interpreted as \\(f(row,col)\\) . display_rows : list / string Default: 'all' Choose which of the dataset's features will be displyed in the output's correlations table rows. If string, can either be a single feature's name or 'all'. Only used if hide_rows is None . display_columns : list / string Default: 'all' Choose which of the dataset's features will be displyed in the output's correlations table columns. If string, can either be a single feature's name or 'all'. Only used if hide_columns is None . hide_rows : list / string Default: None choose which of the dataset's features will not be displyed in the output's correlations table rows. If string, must be a single feature's name. If None , display_rows is used. hide_columns : list / string Default: None choose which of the dataset's features will not be displyed in the output's correlations table columns. If string, must be a single feature's name. If None , display_columns is used. cramers_v_bias_correction : Boolean Default: True Method signature change This replaces the bias_correction flag which was used till version 0.6.6. Use bias correction for Cramer's V from Bergsma and Wicher, Journal of the Korean Statistical Society 42 (2013): 323-328. nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop_samples' to remove samples with missing values, 'drop_features' to remove features (columns) with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace' ax : matplotlib Axe Default: None Matplotlib Axis on which the heat-map will be plotted figsize : (int,int) or None Default: None A Matplotlib figure-size tuple. If None , falls back to Matplotlib's default. Only used if ax=None . annot : Boolean Default: True Plot number annotations on the heat-map fmt : string Default: '.2f' String formatting of annotations cmap : Matplotlib colormap or None Default: None A colormap to be used for the heat-map. If None, falls back to Seaborn's heat-map default sv_color : string Default: 'silver' A Matplotlib color. The color to be used when displaying single-value features over the heat-map cbar : Boolean Default: True Display heat-map's color-bar vmax : float Default: 1.0 Set heat-map vmax option vmin : float or None Default: None Set heat-map vmin option. If set to None , vmin will be chosen automatically between 0 and -1.0, depending on the types of associations used (-1.0 if Pearson's R is used, 0 otherwise) plot : Boolean Default: True Plot a heat-map of the correlation matrix. If False, heat-map will still be drawn, but not shown. The heat-map's ax is part of this function's output. compute_only : Boolean Default: False Use this flag only if you have no need of the plotting at all. This skips the entire plotting mechanism (similar to the old compute_associations method). clustering : Boolean Default: False If True, the computed associations will be sorted into groups by similar correlations title : string or None Default: None Plotted graph title. filename : string or None Default: None If not None, plot will be saved to the given file name. Returns: A dictionary with the following keys: corr : A DataFrame of the correlation/strength-of-association between all features ax : A Matplotlib Axe Example: See examples . cluster_correlations \u00b6 cluster_correlations(corr_mat, indexes=None) Apply agglomerative clustering in order to sort a correlation matrix. Based on this clustering example . corr_mat : Pandas DataFrame A correlation matrix (as output from associations ) indexes : list / NumPy ndarray / Pandas Series A sequence of cluster indexes for sorting. If not present, a clustering is performed. Returns: a sorted correlation matrix ( pd.DataFrame ) cluster indexes based on the original dataset ( list ) Example: >>> assoc = associations ( customers , plot = False ) >>> correlations = assoc [ 'corr' ] >>> correlations , _ = cluster_correlations ( correlations ) compute_associations \u00b6 Deprecated compute_associations was deprecated and removed. Use associations(compute_only=True)['corr'] . conditional_entropy \u00b6 conditional_entropy(x, y, nan_strategy=REPLACE, nan_replace_value=DEFAULT_REPLACE_VALUE, log_base=math.e) Given measurements x and y of random variables \\(X\\) and \\(Y\\) , calculates the conditional entropy of \\(X\\) given \\(Y\\) : \\[ S(X|Y) = - \\sum_{x,y} p(x,y) \\log\\frac{p(x,y)}{p(y)} \\] Read more on Wikipedia . x : list / NumPy ndarray / Pandas Series A sequence of measurements y : list / NumPy ndarray / Pandas Series A sequence of measurements nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop' to remove samples with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace'. log_base : float Default: math.e Specifying base for calculating entropy. Returns: float correlation_ratio \u00b6 correlation_ratio(categories, measurements, nan_strategy=REPLACE, nan_replace_value=DEFAULT_REPLACE_VALUE) Calculates the Correlation Ratio ( \\(\\eta\\) ) for categorical-continuous association: \\[ \\eta = \\sqrt{\\frac{\\sum_x{n_x (\\bar{y}_x - \\bar{y})^2}}{\\sum_{x,i}{(y_{xi}-\\bar{y})^2}}} \\] where \\(n_x\\) is the number of observations in category \\(x\\) , and we define: \\[\\bar{y}_x = \\frac{\\sum_i{y_{xi}}}{n_x} , \\bar{y} = \\frac{\\sum_i{n_x \\bar{y}_x}}{\\sum_x{n_x}}\\] Answers the question - given a continuous value of a measurement, is it possible to know which category is it associated with? Value is in the range [0,1], where 0 means a category cannot be determined by a continuous measurement, and 1 means a category can be determined with absolute certainty. Read more on Wikipedia . categories : list / NumPy ndarray / Pandas Series A sequence of categorical measurements measurements : list / NumPy ndarray / Pandas Series A sequence of continuous measurements nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop' to remove samples with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace'. Returns: float in the range of [0,1] cramers_v \u00b6 cramers_v(x, y, bias_correction=True, nan_strategy=REPLACE, nan_replace_value=DEFAULT_REPLACE_VALUE) Calculates Cramer's V statistic for categorical-categorical association. This is a symmetric coefficient: \\(V(x,y) = V(y,x)\\) . Read more on Wikipedia . Original function taken from this answer on StackOverflow. Cramer's V limitations when applied on skewed or small datasets As the Cramer's V measure of association depends directly on the counts of each samples-pair in the data, it tends to be suboptimal when applied on skewed or small datasets. Consider each of the following cases, where we would expect Cramer's V to reach a high value, yet this only happens in the first scenario: >>> x = [ 'a' ] * 400 + [ 'b' ] * 100 >>> y = [ 'X' ] * 400 + [ 'Y' ] * 100 >>> cramers_v ( x , y ) 0.9937374102534072 # skewed dataset >>> x = [ 'a' ] * 500 + [ 'b' ] * 1 >>> y = [ 'X' ] * 500 + [ 'Y' ] * 1 >>> cramers_v ( x , y ) 0.4974896903293253 # very small dataset >>> x = [ 'a' ] * 4 + [ 'b' ] * 1 >>> y = [ 'X' ] * 4 + [ 'Y' ] * 1 >>> cramers_v ( x , y ) 0.0 x : list / NumPy ndarray / Pandas Series A sequence of categorical measurements y : list / NumPy ndarray / Pandas Series A sequence of categorical measurements bias_correction : Boolean Default: True Use bias correction from Bergsma and Wicher, Journal of the Korean Statistical Society 42 (2013): 323-328. nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop' to remove samples with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace'. Returns: float in the range of [0,1] identify_nominal_columns \u00b6 identify_nominal_columns(dataset) Given a dataset, identify categorical columns. This is used internally in associations and numerical_encoding , but can also be used directly. Note: This is a shortcut for data_utils.identify_columns_by_type(dataset, include=['object', 'category']) dataset : np.ndarray / pd.DataFrame Returns: list of categorical columns Example: >>> df = pd . DataFrame ({ 'col1' : [ 'a' , 'b' , 'c' , 'a' ], 'col2' : [ 3 , 4 , 2 , 1 ]}) >>> identify_nominal_columns ( df ) [ 'col1' ] identify_numeric_columns \u00b6 identify_numeric_columns(dataset) Given a dataset, identify numeric columns. Note: This is a shortcut for data_utils.identify_columns_by_type(dataset, include=['int64', 'float64']) dataset : np.ndarray / pd.DataFrame Returns: list of numerical columns Example: >>> df = pd . DataFrame ({ 'col1' : [ 'a' , 'b' , 'c' , 'a' ], 'col2' : [ 3 , 4 , 2 , 1 ], 'col3' : [ 1. , 2. , 3. , 4. ]}) >>> identify_numeric_columns ( df ) [ 'col2' , 'col3' ] numerical_encoding \u00b6 numerical_encoding(dataset, nominal_columns='auto', drop_single_label=False, drop_fact_dict=True, nan_strategy=REPLACE, nan_replace_value=DEFAULT_REPLACE_VALUE) Encoding a data-set with mixed data (numerical and categorical) to a numerical-only data-set, using the following logic: categorical with only a single value will be marked as zero (or dropped, if requested) categorical with two values will be replaced with the result of Pandas factorize categorical with more than two values will be replaced with the result of Pandas get_dummies numerical columns will not be modified dataset : NumPy ndarray / Pandas DataFrame The data-set to encode nominal_columns : sequence / string Default: 'auto' Names of columns of the data-set which hold categorical values. Can also be the string 'all' to state that all columns are categorical, 'auto' (default) to identify nominal columns automatically, or None to state none are categorical (nothing happens) drop_single_label : Boolean Default: False If True, nominal columns with a only a single value will be dropped. drop_fact_dict : Boolean Default: True If True, the return value will be the encoded DataFrame alone. If False, it will be a tuple of the DataFrame and the dictionary of the binary factorization (originating from pd.factorize) nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop_samples' to remove samples with missing values, 'drop_features' to remove features (columns) with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace' Returns: pd.DataFrame or (pd.DataFrame, dict) . If drop_fact_dict is True, returns the encoded DataFrame. else, returns a tuple of the encoded DataFrame and dictionary, where each key is a two-value column, and the value is the original labels, as supplied by Pandas factorize . Will be empty if no two-value columns are present in the data-set theils_u \u00b6 theils_u(x, y, nan_strategy=REPLACE, nan_replace_value=DEFAULT_REPLACE_VALUE) Calculates Theil's U statistic (Uncertainty coefficient) for categorical-categorical association, defined as: \\[ U(X|Y) = \\frac{S(X) - S(X|Y)}{S(X)} \\] where \\(S(X)\\) is the entropy of \\(X\\) and \\(S(X|Y)\\) is the conditional entropy of \\(X\\) given \\(Y\\) . This is the uncertainty of x given y: value is on the range of [0,1] - where 0 means y provides no information about x, and 1 means y provides full information about x. This is an asymmetric coefficient: \\(U(x,y) \\neq U(y,x)\\) . Read more on Wikipedia . x : list / NumPy ndarray / Pandas Series A sequence of categorical measurements y : list / NumPy ndarray / Pandas Series A sequence of categorical measurements nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop' to remove samples with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace'. Returns: float in the range of [0,1]","title":"nominal"},{"location":"modules/nominal/#nominal","text":"","title":"nominal"},{"location":"modules/nominal/#associations","text":"associations(dataset, nominal_columns='auto', numerical_columns=None, mark_columns=False,nom_nom_assoc='cramer', num_num_assoc='pearson', nom_num_assoc='correlation_ratio', symmetric_nom_nom=True, symmetric_num_num=True, display_rows='all', display_columns='all', hide_rows=None, hide_columns=None, cramers_v_bias_correction=True, nan_strategy=_REPLACE, nan_replace_value=_DEFAULT_REPLACE_VALUE, ax=None, figsize=None, annot=True, fmt='.2f', cmap=None, sv_color='silver', cbar=True, vmax=1.0, vmin=None, plot=True, compute_only=False, clustering=False, title=None, filename=None) Calculate the correlation/strength-of-association of features in data-set with both categorical and continuous features using: * Pearson's R for continuous-continuous cases * Correlation Ratio for categorical-continuous cases * Cramer's V or Theil's U for categorical-categorical cases dataset : NumPy ndarray / Pandas DataFrame The data-set for which the features' correlation is computed nominal_columns : string / list / NumPy ndarray Default: 'auto' Names of columns of the data-set which hold categorical values. Can also be the string 'all' to state that all columns are categorical, 'auto' (default) to identify nominal columns automatically, or None to state none are categorical. Only used if numerical_columns is None . numerical_columns : string / list / NumPy ndarray Default: None To be used instead of nominal_columns . Names of columns of the data-set which hold numerical values. Can also be the string 'all' to state that all columns are numerical (equivalent to nominal_columns=None ) or 'auto' to try to identify numerical columns (equivalent to nominal_columns=auto ). If None , nominal_columns is used. mark_columns : Boolean Default: False if True, output's columns' names will have a suffix of '(nom)' or '(con)' based on their type (nominal or continuous), as provided by nominal_columns nom_nom_assoc : callable / string Default: 'cramer' Method signature change This replaces the theil_u flag which was used till version 0.6.6. If callable, a function which recieves two pd.Series and returns a single number. If string, name of nominal-nominal (categorical-categorical) association to use: cramer : Cramer's V theil : Theil's U. When selected, heat-map columns are the provided information (meaning: \\(U = U(row|col)\\) ) num_num_assoc : callable / string Default: 'pearson' If callable, a function which recieves two pd.Series and returns a single number. If string, name of numerical-numerical association to use: pearson : Pearson's R spearman : Spearman's R kendall : Kendall's Tau nom_num_assoc : callable / string Default: 'correlation_ratio' If callable, a function which recieves two pd.Series and returns a single number. If string, name of nominal-numerical association to use: correlation_ratio : correlation ratio symmetric_nom_nom : Boolean Default: True Relevant only if nom_nom_assoc is a callable. If so, declare whether the function is symmetric ( \\(f(x,y) = f(y,x)\\) ). If False, heat-map values should be interpreted as \\(f(row,col)\\) . symmetric_num_num : Boolean Default: True Relevant only if num_num_assoc is a callable. If so, declare whether the function is symmetric ( \\(f(x,y) = f(y,x)\\) ). If False, heat-map values should be interpreted as \\(f(row,col)\\) . display_rows : list / string Default: 'all' Choose which of the dataset's features will be displyed in the output's correlations table rows. If string, can either be a single feature's name or 'all'. Only used if hide_rows is None . display_columns : list / string Default: 'all' Choose which of the dataset's features will be displyed in the output's correlations table columns. If string, can either be a single feature's name or 'all'. Only used if hide_columns is None . hide_rows : list / string Default: None choose which of the dataset's features will not be displyed in the output's correlations table rows. If string, must be a single feature's name. If None , display_rows is used. hide_columns : list / string Default: None choose which of the dataset's features will not be displyed in the output's correlations table columns. If string, must be a single feature's name. If None , display_columns is used. cramers_v_bias_correction : Boolean Default: True Method signature change This replaces the bias_correction flag which was used till version 0.6.6. Use bias correction for Cramer's V from Bergsma and Wicher, Journal of the Korean Statistical Society 42 (2013): 323-328. nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop_samples' to remove samples with missing values, 'drop_features' to remove features (columns) with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace' ax : matplotlib Axe Default: None Matplotlib Axis on which the heat-map will be plotted figsize : (int,int) or None Default: None A Matplotlib figure-size tuple. If None , falls back to Matplotlib's default. Only used if ax=None . annot : Boolean Default: True Plot number annotations on the heat-map fmt : string Default: '.2f' String formatting of annotations cmap : Matplotlib colormap or None Default: None A colormap to be used for the heat-map. If None, falls back to Seaborn's heat-map default sv_color : string Default: 'silver' A Matplotlib color. The color to be used when displaying single-value features over the heat-map cbar : Boolean Default: True Display heat-map's color-bar vmax : float Default: 1.0 Set heat-map vmax option vmin : float or None Default: None Set heat-map vmin option. If set to None , vmin will be chosen automatically between 0 and -1.0, depending on the types of associations used (-1.0 if Pearson's R is used, 0 otherwise) plot : Boolean Default: True Plot a heat-map of the correlation matrix. If False, heat-map will still be drawn, but not shown. The heat-map's ax is part of this function's output. compute_only : Boolean Default: False Use this flag only if you have no need of the plotting at all. This skips the entire plotting mechanism (similar to the old compute_associations method). clustering : Boolean Default: False If True, the computed associations will be sorted into groups by similar correlations title : string or None Default: None Plotted graph title. filename : string or None Default: None If not None, plot will be saved to the given file name. Returns: A dictionary with the following keys: corr : A DataFrame of the correlation/strength-of-association between all features ax : A Matplotlib Axe Example: See examples .","title":"associations"},{"location":"modules/nominal/#cluster_correlations","text":"cluster_correlations(corr_mat, indexes=None) Apply agglomerative clustering in order to sort a correlation matrix. Based on this clustering example . corr_mat : Pandas DataFrame A correlation matrix (as output from associations ) indexes : list / NumPy ndarray / Pandas Series A sequence of cluster indexes for sorting. If not present, a clustering is performed. Returns: a sorted correlation matrix ( pd.DataFrame ) cluster indexes based on the original dataset ( list ) Example: >>> assoc = associations ( customers , plot = False ) >>> correlations = assoc [ 'corr' ] >>> correlations , _ = cluster_correlations ( correlations )","title":"cluster_correlations"},{"location":"modules/nominal/#compute_associations","text":"Deprecated compute_associations was deprecated and removed. Use associations(compute_only=True)['corr'] .","title":"compute_associations"},{"location":"modules/nominal/#conditional_entropy","text":"conditional_entropy(x, y, nan_strategy=REPLACE, nan_replace_value=DEFAULT_REPLACE_VALUE, log_base=math.e) Given measurements x and y of random variables \\(X\\) and \\(Y\\) , calculates the conditional entropy of \\(X\\) given \\(Y\\) : \\[ S(X|Y) = - \\sum_{x,y} p(x,y) \\log\\frac{p(x,y)}{p(y)} \\] Read more on Wikipedia . x : list / NumPy ndarray / Pandas Series A sequence of measurements y : list / NumPy ndarray / Pandas Series A sequence of measurements nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop' to remove samples with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace'. log_base : float Default: math.e Specifying base for calculating entropy. Returns: float","title":"conditional_entropy"},{"location":"modules/nominal/#correlation_ratio","text":"correlation_ratio(categories, measurements, nan_strategy=REPLACE, nan_replace_value=DEFAULT_REPLACE_VALUE) Calculates the Correlation Ratio ( \\(\\eta\\) ) for categorical-continuous association: \\[ \\eta = \\sqrt{\\frac{\\sum_x{n_x (\\bar{y}_x - \\bar{y})^2}}{\\sum_{x,i}{(y_{xi}-\\bar{y})^2}}} \\] where \\(n_x\\) is the number of observations in category \\(x\\) , and we define: \\[\\bar{y}_x = \\frac{\\sum_i{y_{xi}}}{n_x} , \\bar{y} = \\frac{\\sum_i{n_x \\bar{y}_x}}{\\sum_x{n_x}}\\] Answers the question - given a continuous value of a measurement, is it possible to know which category is it associated with? Value is in the range [0,1], where 0 means a category cannot be determined by a continuous measurement, and 1 means a category can be determined with absolute certainty. Read more on Wikipedia . categories : list / NumPy ndarray / Pandas Series A sequence of categorical measurements measurements : list / NumPy ndarray / Pandas Series A sequence of continuous measurements nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop' to remove samples with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace'. Returns: float in the range of [0,1]","title":"correlation_ratio"},{"location":"modules/nominal/#cramers_v","text":"cramers_v(x, y, bias_correction=True, nan_strategy=REPLACE, nan_replace_value=DEFAULT_REPLACE_VALUE) Calculates Cramer's V statistic for categorical-categorical association. This is a symmetric coefficient: \\(V(x,y) = V(y,x)\\) . Read more on Wikipedia . Original function taken from this answer on StackOverflow. Cramer's V limitations when applied on skewed or small datasets As the Cramer's V measure of association depends directly on the counts of each samples-pair in the data, it tends to be suboptimal when applied on skewed or small datasets. Consider each of the following cases, where we would expect Cramer's V to reach a high value, yet this only happens in the first scenario: >>> x = [ 'a' ] * 400 + [ 'b' ] * 100 >>> y = [ 'X' ] * 400 + [ 'Y' ] * 100 >>> cramers_v ( x , y ) 0.9937374102534072 # skewed dataset >>> x = [ 'a' ] * 500 + [ 'b' ] * 1 >>> y = [ 'X' ] * 500 + [ 'Y' ] * 1 >>> cramers_v ( x , y ) 0.4974896903293253 # very small dataset >>> x = [ 'a' ] * 4 + [ 'b' ] * 1 >>> y = [ 'X' ] * 4 + [ 'Y' ] * 1 >>> cramers_v ( x , y ) 0.0 x : list / NumPy ndarray / Pandas Series A sequence of categorical measurements y : list / NumPy ndarray / Pandas Series A sequence of categorical measurements bias_correction : Boolean Default: True Use bias correction from Bergsma and Wicher, Journal of the Korean Statistical Society 42 (2013): 323-328. nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop' to remove samples with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace'. Returns: float in the range of [0,1]","title":"cramers_v"},{"location":"modules/nominal/#identify_nominal_columns","text":"identify_nominal_columns(dataset) Given a dataset, identify categorical columns. This is used internally in associations and numerical_encoding , but can also be used directly. Note: This is a shortcut for data_utils.identify_columns_by_type(dataset, include=['object', 'category']) dataset : np.ndarray / pd.DataFrame Returns: list of categorical columns Example: >>> df = pd . DataFrame ({ 'col1' : [ 'a' , 'b' , 'c' , 'a' ], 'col2' : [ 3 , 4 , 2 , 1 ]}) >>> identify_nominal_columns ( df ) [ 'col1' ]","title":"identify_nominal_columns"},{"location":"modules/nominal/#identify_numeric_columns","text":"identify_numeric_columns(dataset) Given a dataset, identify numeric columns. Note: This is a shortcut for data_utils.identify_columns_by_type(dataset, include=['int64', 'float64']) dataset : np.ndarray / pd.DataFrame Returns: list of numerical columns Example: >>> df = pd . DataFrame ({ 'col1' : [ 'a' , 'b' , 'c' , 'a' ], 'col2' : [ 3 , 4 , 2 , 1 ], 'col3' : [ 1. , 2. , 3. , 4. ]}) >>> identify_numeric_columns ( df ) [ 'col2' , 'col3' ]","title":"identify_numeric_columns"},{"location":"modules/nominal/#numerical_encoding","text":"numerical_encoding(dataset, nominal_columns='auto', drop_single_label=False, drop_fact_dict=True, nan_strategy=REPLACE, nan_replace_value=DEFAULT_REPLACE_VALUE) Encoding a data-set with mixed data (numerical and categorical) to a numerical-only data-set, using the following logic: categorical with only a single value will be marked as zero (or dropped, if requested) categorical with two values will be replaced with the result of Pandas factorize categorical with more than two values will be replaced with the result of Pandas get_dummies numerical columns will not be modified dataset : NumPy ndarray / Pandas DataFrame The data-set to encode nominal_columns : sequence / string Default: 'auto' Names of columns of the data-set which hold categorical values. Can also be the string 'all' to state that all columns are categorical, 'auto' (default) to identify nominal columns automatically, or None to state none are categorical (nothing happens) drop_single_label : Boolean Default: False If True, nominal columns with a only a single value will be dropped. drop_fact_dict : Boolean Default: True If True, the return value will be the encoded DataFrame alone. If False, it will be a tuple of the DataFrame and the dictionary of the binary factorization (originating from pd.factorize) nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop_samples' to remove samples with missing values, 'drop_features' to remove features (columns) with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace' Returns: pd.DataFrame or (pd.DataFrame, dict) . If drop_fact_dict is True, returns the encoded DataFrame. else, returns a tuple of the encoded DataFrame and dictionary, where each key is a two-value column, and the value is the original labels, as supplied by Pandas factorize . Will be empty if no two-value columns are present in the data-set","title":"numerical_encoding"},{"location":"modules/nominal/#theils_u","text":"theils_u(x, y, nan_strategy=REPLACE, nan_replace_value=DEFAULT_REPLACE_VALUE) Calculates Theil's U statistic (Uncertainty coefficient) for categorical-categorical association, defined as: \\[ U(X|Y) = \\frac{S(X) - S(X|Y)}{S(X)} \\] where \\(S(X)\\) is the entropy of \\(X\\) and \\(S(X|Y)\\) is the conditional entropy of \\(X\\) given \\(Y\\) . This is the uncertainty of x given y: value is on the range of [0,1] - where 0 means y provides no information about x, and 1 means y provides full information about x. This is an asymmetric coefficient: \\(U(x,y) \\neq U(y,x)\\) . Read more on Wikipedia . x : list / NumPy ndarray / Pandas Series A sequence of categorical measurements y : list / NumPy ndarray / Pandas Series A sequence of categorical measurements nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop' to remove samples with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace'. Returns: float in the range of [0,1]","title":"theils_u"},{"location":"modules/sampling/","text":"sampling \u00b6 boltzmann_sampling \u00b6 boltzmann_sampling(numbers, k=1, with_replacement=False) Return k numbers from a boltzmann-sampling over the supplied numbers numbers : List or np.ndarray numbers to sample k : int Default: 1 How many numbers to sample. Choosing k=None will yield a single number with_replacement : Boolean Default: False Allow replacement or not Returns: list , np.ndarray or a single number (depending on the input) weighted_sampling \u00b6 weighted_sampling(numbers, k=1, with_replacement=False) Return k numbers from a weighted-sampling over the supplied numbers numbers : List or np.ndarray numbers to sample k : int Default: 1 How many numbers to sample. Choosing k=None will yield a single number with_replacement : Boolean Default: False Allow replacement or not Returns: list , np.ndarray or a single number (depending on the input)","title":"sampling"},{"location":"modules/sampling/#sampling","text":"","title":"sampling"},{"location":"modules/sampling/#boltzmann_sampling","text":"boltzmann_sampling(numbers, k=1, with_replacement=False) Return k numbers from a boltzmann-sampling over the supplied numbers numbers : List or np.ndarray numbers to sample k : int Default: 1 How many numbers to sample. Choosing k=None will yield a single number with_replacement : Boolean Default: False Allow replacement or not Returns: list , np.ndarray or a single number (depending on the input)","title":"boltzmann_sampling"},{"location":"modules/sampling/#weighted_sampling","text":"weighted_sampling(numbers, k=1, with_replacement=False) Return k numbers from a weighted-sampling over the supplied numbers numbers : List or np.ndarray numbers to sample k : int Default: 1 How many numbers to sample. Choosing k=None will yield a single number with_replacement : Boolean Default: False Allow replacement or not Returns: list , np.ndarray or a single number (depending on the input)","title":"weighted_sampling"}]}